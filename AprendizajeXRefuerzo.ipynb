{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1 - Aprendizaje Por Refuerzos\n",
    "## Integrantes:\n",
    "- **Azul Noguera** \n",
    "- **Paula Jordan** \n",
    "- **Rocio Gonzalez**\n",
    "- **Valentina Pancaldi**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enunciado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar un agente de aprendizaje por refuerzo y un ambiente en el cual el agente puede entrenarse. Esto incluye definir la noción de estado del ambiente, que puede no coincidir con el estado del juego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación del Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos las clases\n",
    "from utils import *\n",
    "from jugador import * # importamos la clase Jugador\n",
    "from diezmil import * # importamos la clase DiezMil\n",
    "\n",
    "# importamos funciones de utilidad\n",
    "from utils import puntaje_y_no_usados, separar, JUGADA_PLANTARSE, JUGADA_TIRAR, JUGADAS_STR\n",
    "\n",
    "# importamos las librerias necesarias\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from random import randint\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EstadoDiezMil:\n",
    "    def __init__(self):\n",
    "        \"\"\"Definir qué hace a un estado de diez mil.\n",
    "        Recordar que la complejidad del estado repercute en la complejidad de la tabla del agente de q-learning.\n",
    "        \"\"\"\n",
    "        self.dados = []  # Lista de los valores actuales de los dados\n",
    "        self.puntaje_actual = 0  # Puntaje acumulado en el turno actual\n",
    "        self.turno_terminado = False  # Flag para indicar si el turno ha terminado\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reinicia el estado del juego a sus valores iniciales.\"\"\"\n",
    "        self.dados = self.lanzar_dados(6)  # Reiniciar los dados\n",
    "        self.puntaje_actual = 0  # Reiniciar el puntaje acumulado\n",
    "        self.turno_terminado = False  # Reiniciar el flag de turno terminado\n",
    "\n",
    "    def lanzar_dados(self, cantidad: int) -> list[int]:\n",
    "        \"\"\"Simula el lanzamiento de una cantidad específica de dados.\n",
    "        Args:\n",
    "            cantidad (int): La cantidad de dados a lanzar.\n",
    "        Returns:\n",
    "            list[int]: Los valores obtenidos en los dados lanzados.\n",
    "        \"\"\"\n",
    "        return [randint(1, 6) for _ in range(cantidad)]\n",
    "\n",
    "    def obtener_estado(self):\n",
    "        \"\"\"Devuelve el estado actual del juego en un formato que el agente pueda utilizar.\"\"\"\n",
    "        return (self.dados, self.puntaje_actual, self.turno_terminado)\n",
    "\n",
    "    def actualizar_estado(self, nuevos_dados: list[int], puntaje: int) -> None:\n",
    "        \"\"\"Modifica las variables internas del estado luego de una tirada.\n",
    "        Args:\n",
    "            nuevos_dados (list[int]): Dados no usados tras la tirada.\n",
    "            puntaje (int): Puntaje obtenido en la tirada.\n",
    "        \"\"\"\n",
    "    \n",
    "        self.dados = nuevos_dados\n",
    "        self.puntaje_actual += puntaje\n",
    "        if len(self.dados) == 0:\n",
    "            # Si no quedan dados no usados, el turno debe terminar\n",
    "            self.turno_terminado = True\n",
    "    \n",
    "    def fin_turno(self):\n",
    "        \"\"\"Modifica el estado al terminar el turno.\n",
    "        \"\"\"\n",
    "        self.turno_terminado = True  # Marca el turno como terminado\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Representación en texto de EstadoDiezMil.\n",
    "        Ayuda a tener una versión legible del objeto.\n",
    "\n",
    "        Returns:\n",
    "            str: Representación en texto de EstadoDiezMil.\n",
    "        \"\"\"\n",
    "        estado_str = f\"Dados: {self.dados}, Puntaje Actual: {self.puntaje_actual}, Turno Terminado: {self.turno_terminado}\"\n",
    "        return estado_str   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmbienteDiezMil:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Definir las variables de instancia de un ambiente.\n",
    "        ¿Qué es propio de un ambiente de 10.000?\n",
    "        \"\"\"\n",
    "        self.estado = EstadoDiezMil()\n",
    "        self.turno_terminado = False  # flag que indica si el turno terminó\n",
    "        self.puntaje_total = 0  # Mantener un registro del puntaje total\n",
    "        self.pasos_totales = 0  # Contador de pasos\n",
    "        self.episodios_totales = 0  # Contador de episodios\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reinicia el estado del ambiente.\n",
    "        \"\"\"\n",
    "        # if self.pasos_totales > 0:  # Solo sumar episodios si se han realizado pasos\n",
    "        self.estado.reset()\n",
    "        self.turno_terminado = False\n",
    "        \n",
    "        self.episodios_totales += 1\n",
    "        self.pasos_totales = 0\n",
    "        \n",
    "        # Aquí no reseteamos `self.puntaje_total` para mantener el acumulado entre episodios.\n",
    "        return self.estado.obtener_estado()\n",
    "    \n",
    "    def step(self, accion, verbose):\n",
    "        \"\"\"\n",
    "        Dada una acción devuelve una recompensa.\n",
    "        El estado es modificado acorde a la acción y su interacción con el ambiente.\n",
    "        Podría ser útil devolver si terminó o no el turno.\n",
    "        \n",
    "        Args:\n",
    "            accion: Acción elegida por un agente.\n",
    "        \n",
    "        Returns:\n",
    "            tuple[int, bool]: Una recompensa y un flag que indica si terminó el turno. \n",
    "        \"\"\"\n",
    "        if verbose:   \n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Acción Elegida: {accion.upper()}\")\n",
    "            print(f\"Estado Antes de la Acción: {self.estado}\")\n",
    "        \n",
    "        self.pasos_totales += 1  # Incrementar el contador de pasos en cada acción\n",
    "        \n",
    "        if verbose:   \n",
    "            print(f\"Paso {self.pasos_totales}: Acción = {accion}, Puntaje Total = {self.puntaje_total}\")\n",
    "        \n",
    "        if accion == 'plantarse':\n",
    "            self.turno_terminado = True\n",
    "            recompensa = self.estado.puntaje_actual\n",
    "            # self.puntaje_total += recompensa   # Acumular la recompensa al puntaje total\n",
    "            \n",
    "        elif accion == 'tirar':\n",
    "            if len(self.estado.dados) != 6:\n",
    "                self.estado.dados = self.estado.lanzar_dados(len(self.estado.dados))\n",
    "            \n",
    "            puntaje, dados_no_usados = puntaje_y_no_usados(self.estado.dados)\n",
    "            if verbose:   \n",
    "                print(f\"Tirada de Dados: {self.estado.dados}\")\n",
    "                print(f\"Puntaje Calculado: {puntaje}, Dados No Usados: {dados_no_usados}\")\n",
    "            \n",
    "            self.estado.actualizar_estado(dados_no_usados, puntaje)\n",
    "            recompensa = self.estado.puntaje_actual\n",
    "            self.puntaje_total += recompensa  # Acumular la recompensa al puntaje total\n",
    "            \n",
    "            if recompensa == 0:\n",
    "                self.turno_terminado = True\n",
    "            \n",
    "            # Lanzamos los dados solo si no es el primer paso\n",
    "            if len(self.estado.dados) == 0:\n",
    "                self.estado.dados = self.estado.lanzar_dados(6)\n",
    "         \n",
    "        if verbose:   \n",
    "            print(f\"Estado Después de la Acción: {self.estado}\")\n",
    "            print(f\"Recompensa Obtenida: {recompensa}\")\n",
    "            print(f\"Puntaje Total Acumulado: {self.puntaje_total}\")\n",
    "            print(f\"{'='*50}\\n\")\n",
    "        \n",
    "        return self.estado.obtener_estado(), recompensa, self.turno_terminado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgenteQLearning:\n",
    "    def __init__(self, ambiente: AmbienteDiezMil, alpha: float, gamma: float, epsilon: float):\n",
    "        self.ambiente = ambiente\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = {}\n",
    "\n",
    "    def elegir_accion(self, estado):\n",
    "        # Convertir cada parte del estado a una tupla si es necesario\n",
    "        estado = tuple(tuple(part) if isinstance(part, list) else part for part in estado)\n",
    "        \n",
    "        if len(self.ambiente.estado.dados) == 6:\n",
    "            return 'tirar'\n",
    "            \n",
    "        else:\n",
    "            if random.uniform(0, 1) < self.epsilon:\n",
    "                return random.choice(['plantarse', 'tirar'])\n",
    "            else:\n",
    "                if estado in self.q_table:\n",
    "                    return max(self.q_table[estado], key=self.q_table[estado].get)\n",
    "                else:\n",
    "                    return random.choice(['plantarse', 'tirar'])\n",
    "\n",
    "    def entrenar(self, episodios: int, verbose: bool = False) -> None:\n",
    "        for episodio in tqdm(range(episodios)):\n",
    "            \n",
    "            # Verificar la condición de finalización del juego\n",
    "            if self.ambiente.puntaje_total >= 10000:\n",
    "                print(f\"Entrenamiento terminado en el episodio {episodio + 1} con un puntaje total de {self.ambiente.puntaje_total}.\")\n",
    "                break\n",
    "            \n",
    "            estado = self.ambiente.reset()\n",
    "            estado = tuple(tuple(part) if isinstance(part, list) else part for part in estado)\n",
    "            terminado = False\n",
    "\n",
    "            while not terminado:\n",
    "                accion = self.elegir_accion(estado)\n",
    "                nuevo_estado, recompensa, terminado = self.ambiente.step(accion, verbose=False)\n",
    "                nuevo_estado = tuple(tuple(part) if isinstance(part, list) else part for part in nuevo_estado)\n",
    "\n",
    "                if estado not in self.q_table:\n",
    "                    self.q_table[estado] = {'plantarse': 0, 'tirar': 0}\n",
    "                \n",
    "                if nuevo_estado not in self.q_table:\n",
    "                    self.q_table[nuevo_estado] = {'plantarse': 0, 'tirar': 0}\n",
    "                    \n",
    "                mejor_q_nuevo_estado = max(self.q_table[nuevo_estado].values())\n",
    "                self.q_table[estado][accion] += self.alpha * (\n",
    "                    recompensa + self.gamma * mejor_q_nuevo_estado - self.q_table[estado][accion]\n",
    "                )\n",
    "                if verbose:\n",
    "                    print(f\"Estado guardado: {estado}, Plantarse/Tirar: {self.q_table[estado]}\")\n",
    "\n",
    "                estado = nuevo_estado\n",
    "                \n",
    "                if self.ambiente.puntaje_total >= 10000:\n",
    "                    terminado = True\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Paso: {episodio + 1}, Estado: {estado}, Acción: {accion}, Recompensa: {recompensa}\")\n",
    "\n",
    "\n",
    "    def guardar_politica(self, filename: str):\n",
    "        # Cargar los estados existentes en el archivo, basado en la cantidad de dados no usados y el puntaje acumulado\n",
    "        estados_existentes = {}\n",
    "        try:\n",
    "            with open(filename, 'r', newline='') as file:\n",
    "                reader = csv.reader(file)\n",
    "                next(reader)  # Saltar el encabezado\n",
    "                for row in reader:\n",
    "                    estado_str = row[0]  # El estado (cantidad de dados y puntaje) está en la primera columna\n",
    "                    plantarse = float(row[1])\n",
    "                    tirar = float(row[2])\n",
    "                    # Convertir el estado en cadena de vuelta a su formato original (evaluación)\n",
    "                    estado_eval = eval(estado_str)\n",
    "                    cantidad_dados, puntaje_acumulado = estado_eval  # Extraer cantidad de dados y puntaje\n",
    "                    # Usamos la cantidad de dados y puntaje como clave\n",
    "                    estados_existentes[(cantidad_dados, puntaje_acumulado)] = {'plantarse': plantarse, 'tirar': tirar}\n",
    "                    \n",
    "        except FileNotFoundError:\n",
    "            # Si el archivo no existe, continuar sin cargar estados\n",
    "            pass\n",
    "\n",
    "        # Actualizar los valores de la política con los de la q_table\n",
    "        for estado, acciones in self.q_table.items():\n",
    "            # Calcular la cantidad de dados no usados y el puntaje actual\n",
    "            cantidad_dados_no_usados = len(estado[0])  # Asumimos que el primer elemento del estado es la lista de dados\n",
    "            puntaje_actual = estado[1]  # Asumimos que el segundo elemento del estado es el puntaje acumulado\n",
    "\n",
    "            clave_estado = (cantidad_dados_no_usados, puntaje_actual)  # Crear la nueva clave basada en la cantidad de dados y puntaje\n",
    "\n",
    "            if clave_estado in estados_existentes:\n",
    "                # Si el estado ya existe, acumular las recompensas\n",
    "                for accion in ['plantarse', 'tirar']:\n",
    "                    # Acumular las recompensas usando la fórmula\n",
    "                    estados_existentes[clave_estado][accion] += self.alpha * (\n",
    "                        acciones[accion] + self.gamma * max(estados_existentes[clave_estado].values())\n",
    "                        - estados_existentes[clave_estado][accion]\n",
    "                    )\n",
    "            else:\n",
    "                # Si es un estado nuevo, agregarlo a estados_existentes\n",
    "                estados_existentes[clave_estado] = acciones\n",
    "\n",
    "        # Abrir el archivo en modo de escritura para actualizar el archivo sin duplicados\n",
    "        with open(filename, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            # Escribir encabezado\n",
    "            writer.writerow(['estado', 'plantarse', 'tirar'])\n",
    "\n",
    "            # Reescribir el archivo con todos los estados, actualizados o no\n",
    "            for clave_estado, acciones in estados_existentes.items():\n",
    "                writer.writerow([str(clave_estado), acciones['plantarse'], acciones['tirar']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JugadorEntrenado(Jugador):\n",
    "    def __init__(self, nombre: str, filename_politica: str):\n",
    "        self.nombre = nombre\n",
    "        self.politica = self._leer_politica(filename_politica)\n",
    "\n",
    "    def _leer_politica(self, filename: str, SEP: str = ','):\n",
    "        \"\"\"Carga una política entrenada de un archivo CSV y la almacena en un diccionario.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre/Path del archivo que contiene a una política almacenada.\n",
    "        \"\"\"\n",
    "        politica = {}\n",
    "        try:\n",
    "            with open(filename, 'r', newline='') as file:\n",
    "                reader = csv.reader(file, delimiter=SEP)\n",
    "                next(reader)  # Saltar el encabezado\n",
    "                for row in reader:\n",
    "                    estado_str = row[0]  # El estado (cantidad de dados y puntaje)\n",
    "                    plantarse = float(row[1])  # Valor para la acción de plantarse\n",
    "                    tirar = float(row[2])  # Valor para la acción de tirar\n",
    "                    \n",
    "                    # Convertir el estado de cadena a tupla (cantidad de dados, puntaje acumulado)\n",
    "                    estado = eval(estado_str)\n",
    "                    politica[estado] = {'plantarse': plantarse, 'tirar': tirar}\n",
    "                    \n",
    "        except FileNotFoundError:\n",
    "            print(f\"El archivo {filename} no se encontró.\")\n",
    "        return politica\n",
    "\n",
    "    def jugar(self, puntaje_total: int, puntaje_turno: int, dados: list[int]) -> tuple[int, list[int]]:\n",
    "        \"\"\"\n",
    "        Devuelve una jugada (plantarse o tirar) y los dados a tirar.\n",
    "        \n",
    "        Args:\n",
    "            puntaje_total (int): Puntaje total del jugador en la partida.\n",
    "            puntaje_turno (int): Puntaje acumulado en el turno del jugador.\n",
    "            dados (list[int]): Tirada del turno actual.\n",
    "\n",
    "        Returns:\n",
    "            tuple[int, list[int]]: Acción a realizar y la lista de dados a tirar.\n",
    "        \"\"\"\n",
    "        # Calcular la cantidad de dados no usados\n",
    "        cantidad_dados_no_usados = len(dados)\n",
    "\n",
    "        # Crear la clave para buscar en la política (cantidad de dados no usados, puntaje_turno)\n",
    "        estado_clave = (cantidad_dados_no_usados, puntaje_turno)\n",
    "\n",
    "        # Consultar la política para determinar la acción\n",
    "        if estado_clave in self.politica:\n",
    "            acciones = self.politica[estado_clave]\n",
    "            # Tomar la acción con el valor más alto\n",
    "            accion = max(acciones, key=acciones.get)\n",
    "        else:\n",
    "            print(f\"Estado {estado_clave} no encontrado en la política, usando acción por defecto.\")\n",
    "            accion = 'plantarse'  # Acción por defecto si no se encuentra el estado\n",
    "\n",
    "        # Realizar la acción correspondiente\n",
    "        if accion == 'plantarse':\n",
    "            return ('plantarse', [])\n",
    "        elif accion == 'tirar':\n",
    "            # Aquí puedes calcular los dados no usados\n",
    "            # Si decides tirar, devolver los dados no usados\n",
    "            puntaje, no_usados = puntaje_y_no_usados(dados)\n",
    "            return ('tirar', no_usados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución de Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hiperparametros:\n",
    "- alpha=0.1\n",
    "- gamma=0.9\n",
    "- epsilon=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Codigo utilizado para generar CSV con politica de entrenamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Definir el número de partidas\n",
    "# numero_partidas = 1000\n",
    "\n",
    "# for _ in tqdm(range(numero_partidas)):\n",
    "#     # Crear una nueva instancia del ambiente y del agente para cada partida\n",
    "#     ambiente = AmbienteDiezMil()\n",
    "#     agente = AgenteQLearning(ambiente, alpha=0.1, gamma=0.9, epsilon=0.2)\n",
    "    \n",
    "#     # Entrenar el agente en una partida de 100 episodios\n",
    "#     agente.entrenar(episodios=100, verbose=False)\n",
    "\n",
    "#     agente.guardar_politica(\"politica_qlearning.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "turno 1: 335515 T(33)  65 P --> 650 puntos. TOTAL: 650\n",
      "turno 2: 435463 T(33446)  26222 P --> 250 puntos. TOTAL: 900\n",
      "turno 3: 561526 T(266)  511 P --> 450 puntos. TOTAL: 1350\n",
      "turno 4: 211143 T(234)  625 P --> 1050 puntos. TOTAL: 2400\n",
      "turno 5: 644644 T(123456)  434543 T(33)  45 P --> 2000 puntos. TOTAL: 4400\n",
      "turno 6: 331461 T(3346)  3162 P --> 300 puntos. TOTAL: 4700\n",
      "turno 7: 146411 T(446)  152 P --> 1150 puntos. TOTAL: 5850\n",
      "turno 8: 124635 T(123456)  644631 T(34466)  52162 P --> 3250 puntos. TOTAL: 9100\n",
      "turno 9: 645144 T(6)  6  --> 0 puntos. TOTAL: 9100\n",
      "turno 10: 415213 T(234)  664  --> 0 puntos. TOTAL: 9100\n",
      "turno 11: 142254 T(2244)  5636 P --> 200 puntos. TOTAL: 9300\n",
      "turno 12: 155162 T(26)  42  --> 0 puntos. TOTAL: 9300\n",
      "turno 13: 425231 T(2234)  1255 P --> 350 puntos. TOTAL: 9650\n",
      "turno 14: 411113 T(34)  26  --> 0 puntos. TOTAL: 9650\n",
      "turno 15: 116521 T(26)  64  --> 0 puntos. TOTAL: 9650\n",
      "turno 16: 636362 T(233)  631 P --> 700 puntos. TOTAL: 10350\n",
      "Cantidad de turnos: 16\n",
      "Puntaje final: 10350\n"
     ]
    }
   ],
   "source": [
    "jugador = JugadorEntrenado('Pepito', 'politica_qlearning.csv')\n",
    "juego = JuegoDiezMil(jugador)\n",
    "cantidad_turnos, puntaje_final = juego.jugar(verbose=True)\n",
    "\n",
    "print(f\"Cantidad de turnos: {cantidad_turnos}\")\n",
    "print(f\"Puntaje final: {puntaje_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado (5, 1150) no encontrado en la política, usando acción por defecto.\n",
      "Estado (5, 6050) no encontrado en la política, usando acción por defecto.\n",
      "Estado (5, 2350) no encontrado en la política, usando acción por defecto.\n",
      "Estado (5, 1650) no encontrado en la política, usando acción por defecto.\n",
      "Estado (4, 3650) no encontrado en la política, usando acción por defecto.\n",
      "Estado (5, 2300) no encontrado en la política, usando acción por defecto.\n",
      "Estado (6, 5400) no encontrado en la política, usando acción por defecto.\n",
      "Estado (2, 3050) no encontrado en la política, usando acción por defecto.\n",
      "Estado (6, 4300) no encontrado en la política, usando acción por defecto.\n",
      "Estado (2, 7050) no encontrado en la política, usando acción por defecto.\n",
      "Estado (2, 5200) no encontrado en la política, usando acción por defecto.\n",
      "Estado (4, 2450) no encontrado en la política, usando acción por defecto.\n",
      "Estado (6, 1800) no encontrado en la política, usando acción por defecto.\n",
      "Estado (3, 5500) no encontrado en la política, usando acción por defecto.\n",
      "Estado (6, 3700) no encontrado en la política, usando acción por defecto.\n",
      "Estado (5, 4450) no encontrado en la política, usando acción por defecto.\n",
      "Estado (6, 3700) no encontrado en la política, usando acción por defecto.\n",
      "Estado (2, 5000) no encontrado en la política, usando acción por defecto.\n",
      "Estado (2, 2400) no encontrado en la política, usando acción por defecto.\n",
      "Estado (4, 1750) no encontrado en la política, usando acción por defecto.\n",
      "Estado (6, 1550) no encontrado en la política, usando acción por defecto.\n",
      "Estado (5, 1450) no encontrado en la política, usando acción por defecto.\n",
      "Estado (3, 4450) no encontrado en la política, usando acción por defecto.\n",
      "Estado (1, 2500) no encontrado en la política, usando acción por defecto.\n",
      "Estado (6, 1050) no encontrado en la política, usando acción por defecto.\n",
      "Estado (3, 5000) no encontrado en la política, usando acción por defecto.\n",
      "Estado (4, 1850) no encontrado en la política, usando acción por defecto.\n",
      "Estado (6, 5400) no encontrado en la política, usando acción por defecto.\n",
      "Estado (1, 6450) no encontrado en la política, usando acción por defecto.\n",
      "Estado (3, 4500) no encontrado en la política, usando acción por defecto.\n",
      "Estado (5, 2350) no encontrado en la política, usando acción por defecto.\n",
      "Estado (6, 4600) no encontrado en la política, usando acción por defecto.\n",
      "Estado (2, 7050) no encontrado en la política, usando acción por defecto.\n",
      "Estado (4, 2200) no encontrado en la política, usando acción por defecto.\n",
      "Estado (6, 1850) no encontrado en la política, usando acción por defecto.\n",
      "Estado (2, 6650) no encontrado en la política, usando acción por defecto.\n",
      "Estado (4, 2000) no encontrado en la política, usando acción por defecto.\n",
      "Estado (4, 2200) no encontrado en la política, usando acción por defecto.\n",
      "El promedio de la cantidad de turnos en 1000 juegos es: 26.658\n"
     ]
    }
   ],
   "source": [
    "# Definir variables acumuladoras\n",
    "total_turnos = 0\n",
    "numero_simulaciones = 1000\n",
    "\n",
    "# Bucle para correr el juego 1000 veces\n",
    "for _ in range(numero_simulaciones):\n",
    "    jugador = JugadorEntrenado('Pepito', 'politica_qlearning.csv')\n",
    "    juego = JuegoDiezMil(jugador)\n",
    "    \n",
    "    # Jugar una partida\n",
    "    cantidad_turnos, puntaje_final = juego.jugar(verbose=False) \n",
    "    \n",
    "    # Sumar la cantidad de turnos al total\n",
    "    total_turnos += cantidad_turnos\n",
    "\n",
    "# Calcular el promedio de turnos\n",
    "promedio_turnos = total_turnos / numero_simulaciones\n",
    "\n",
    "# Imprimir el resultado\n",
    "print(f\"El promedio de la cantidad de turnos en {numero_simulaciones} juegos es: {promedio_turnos}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
