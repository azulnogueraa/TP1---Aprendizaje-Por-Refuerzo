{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1 - Aprendizaje Por Refuerzos\n",
    "## Integrantes:\n",
    "- **Azul Noguera** \n",
    "- **Paula Jordan** \n",
    "- **Rocio Gonzalez**\n",
    "- **Valentina Pancaldi**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enunciado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar un agente de aprendizaje por refuerzo y un ambiente en el cual el agente puede entrenarse. Esto incluye definir la noción de estado del ambiente, que puede no coincidir con el estado del juego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación del Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos las clases\n",
    "from utils import *\n",
    "from jugador import * # importamos la clase Jugador\n",
    "from diezmil import * # importamos la clase DiezMil\n",
    "\n",
    "# importamos funciones de utilidad\n",
    "from utils import puntaje_y_no_usados, separar, JUGADA_PLANTARSE, JUGADA_TIRAR, JUGADAS_STR\n",
    "\n",
    "# importamos las librerias necesarias\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from random import randint\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EstadoDiezMil:\n",
    "    def __init__(self):\n",
    "        \"\"\"Definir qué hace a un estado de diez mil.\n",
    "        Recordar que la complejidad del estado repercute en la complejidad de la tabla del agente de q-learning.\n",
    "        \"\"\"\n",
    "        self.dados = []  # Lista de los valores actuales de los dados\n",
    "        self.puntaje_actual = 0  # Puntaje acumulado en el turno actual\n",
    "        self.turno_terminado = False  # Flag para indicar si el turno ha terminado\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reinicia el estado del juego a sus valores iniciales.\"\"\"\n",
    "        self.dados = self.lanzar_dados(6)  # Reiniciar los dados\n",
    "        self.puntaje_actual = 0  # Reiniciar el puntaje acumulado\n",
    "        self.turno_terminado = False  # Reiniciar el flag de turno terminado\n",
    "\n",
    "    def lanzar_dados(self, cantidad: int) -> list[int]:\n",
    "        \"\"\"Simula el lanzamiento de una cantidad específica de dados.\n",
    "        Args:\n",
    "            cantidad (int): La cantidad de dados a lanzar.\n",
    "        Returns:\n",
    "            list[int]: Los valores obtenidos en los dados lanzados.\n",
    "        \"\"\"\n",
    "        return [randint(1, 6) for _ in range(cantidad)]\n",
    "\n",
    "    def obtener_estado(self):\n",
    "        \"\"\"Devuelve el estado actual del juego en un formato que el agente pueda utilizar.\"\"\"\n",
    "        return (self.dados, self.puntaje_actual, self.turno_terminado)\n",
    "\n",
    "    def actualizar_estado(self, nuevos_dados: list[int], puntaje: int) -> None:\n",
    "        \"\"\"Modifica las variables internas del estado luego de una tirada.\n",
    "        Args:\n",
    "            nuevos_dados (list[int]): Dados no usados tras la tirada.\n",
    "            puntaje (int): Puntaje obtenido en la tirada.\n",
    "        \"\"\"\n",
    "    \n",
    "        self.dados = nuevos_dados\n",
    "        self.puntaje_actual += puntaje\n",
    "        if len(self.dados) == 0:\n",
    "            # Si no quedan dados no usados, el turno debe terminar\n",
    "            self.turno_terminado = True\n",
    "    \n",
    "    def fin_turno(self):\n",
    "        \"\"\"Modifica el estado al terminar el turno.\n",
    "        \"\"\"\n",
    "        self.turno_terminado = True  # Marca el turno como terminado\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Representación en texto de EstadoDiezMil.\n",
    "        Ayuda a tener una versión legible del objeto.\n",
    "\n",
    "        Returns:\n",
    "            str: Representación en texto de EstadoDiezMil.\n",
    "        \"\"\"\n",
    "        estado_str = f\"Dados: {self.dados}, Puntaje Actual: {self.puntaje_actual}, Turno Terminado: {self.turno_terminado}\"\n",
    "        return estado_str   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmbienteDiezMil:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Definir las variables de instancia de un ambiente.\n",
    "        ¿Qué es propio de un ambiente de 10.000?\n",
    "        \"\"\"\n",
    "        self.estado = EstadoDiezMil()\n",
    "        self.turno_terminado = False  # flag que indica si el turno terminó\n",
    "        self.puntaje_total = 0  # Mantener un registro del puntaje total\n",
    "        self.pasos_totales = 0  # Contador de pasos\n",
    "        self.episodios_totales = 0  # Contador de episodios\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reinicia el estado del ambiente.\n",
    "        \"\"\"\n",
    "        # if self.pasos_totales > 0:  # Solo sumar episodios si se han realizado pasos\n",
    "        self.estado.reset()\n",
    "        self.turno_terminado = False\n",
    "        \n",
    "        self.episodios_totales += 1\n",
    "        self.pasos_totales = 0\n",
    "        \n",
    "        # Aquí no reseteamos `self.puntaje_total` para mantener el acumulado entre episodios.\n",
    "        return self.estado.obtener_estado()\n",
    "    \n",
    "    def step(self, accion, verbose):\n",
    "        \"\"\"\n",
    "        Dada una acción devuelve una recompensa.\n",
    "        El estado es modificado acorde a la acción y su interacción con el ambiente.\n",
    "        Podría ser útil devolver si terminó o no el turno.\n",
    "        \n",
    "        Args:\n",
    "            accion: Acción elegida por un agente.\n",
    "        \n",
    "        Returns:\n",
    "            tuple[int, bool]: Una recompensa y un flag que indica si terminó el turno. \n",
    "        \"\"\"\n",
    "        if verbose:   \n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Acción Elegida: {accion.upper()}\")\n",
    "            print(f\"Estado Antes de la Acción: {self.estado}\")\n",
    "        \n",
    "        self.pasos_totales += 1  # Incrementar el contador de pasos en cada acción\n",
    "        \n",
    "        if verbose:   \n",
    "            print(f\"Paso {self.pasos_totales}: Acción = {accion}, Puntaje Total = {self.puntaje_total}\")\n",
    "        \n",
    "        if accion == 'plantarse':\n",
    "            self.turno_terminado = True\n",
    "            recompensa = self.estado.puntaje_actual\n",
    "            # self.puntaje_total += recompensa   # Acumular la recompensa al puntaje total\n",
    "            \n",
    "        elif accion == 'tirar':\n",
    "            if len(self.estado.dados) != 6:\n",
    "                self.estado.dados = self.estado.lanzar_dados(len(self.estado.dados))\n",
    "            \n",
    "            puntaje, dados_no_usados = puntaje_y_no_usados(self.estado.dados)\n",
    "            if verbose:   \n",
    "                print(f\"Tirada de Dados: {self.estado.dados}\")\n",
    "                print(f\"Puntaje Calculado: {puntaje}, Dados No Usados: {dados_no_usados}\")\n",
    "            \n",
    "            self.estado.actualizar_estado(dados_no_usados, puntaje)\n",
    "            recompensa = self.estado.puntaje_actual\n",
    "            self.puntaje_total += recompensa  # Acumular la recompensa al puntaje total\n",
    "            \n",
    "            if recompensa == 0:\n",
    "                self.turno_terminado = True\n",
    "            \n",
    "            # Lanzamos los dados solo si no es el primer paso\n",
    "            if len(self.estado.dados) == 0:\n",
    "                self.estado.dados = self.estado.lanzar_dados(6)\n",
    "         \n",
    "        if verbose:   \n",
    "            print(f\"Estado Después de la Acción: {self.estado}\")\n",
    "            print(f\"Recompensa Obtenida: {recompensa}\")\n",
    "            print(f\"Puntaje Total Acumulado: {self.puntaje_total}\")\n",
    "            print(f\"{'='*50}\\n\")\n",
    "        \n",
    "        return self.estado.obtener_estado(), recompensa, self.turno_terminado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgenteQLearning:\n",
    "    def __init__(self, ambiente: AmbienteDiezMil, alpha: float, gamma: float, epsilon: float):\n",
    "        self.ambiente = ambiente\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = {}\n",
    "\n",
    "    def elegir_accion(self, estado):\n",
    "        # Convertir cada parte del estado a una tupla si es necesario\n",
    "        estado = tuple(tuple(part) if isinstance(part, list) else part for part in estado)\n",
    "        \n",
    "        if len(self.ambiente.estado.dados) == 6:\n",
    "            return 'tirar'\n",
    "            \n",
    "        else:\n",
    "            if random.uniform(0, 1) < self.epsilon:\n",
    "                return random.choice(['plantarse', 'tirar'])\n",
    "            else:\n",
    "                if estado in self.q_table:\n",
    "                    return max(self.q_table[estado], key=self.q_table[estado].get)\n",
    "                else:\n",
    "                    return random.choice(['plantarse', 'tirar'])\n",
    "\n",
    "    def entrenar(self, episodios: int, verbose: bool = False) -> None:\n",
    "        for episodio in tqdm(range(episodios)):\n",
    "            \n",
    "            # Verificar la condición de finalización del juego\n",
    "            if self.ambiente.puntaje_total >= 10000:\n",
    "                print(f\"Entrenamiento terminado en el episodio {episodio + 1} con un puntaje total de {self.ambiente.puntaje_total}.\")\n",
    "                break\n",
    "            \n",
    "            estado = self.ambiente.reset()\n",
    "            estado = tuple(tuple(part) if isinstance(part, list) else part for part in estado)\n",
    "            terminado = False\n",
    "\n",
    "            while not terminado:\n",
    "                accion = self.elegir_accion(estado)\n",
    "                nuevo_estado, recompensa, terminado = self.ambiente.step(accion, verbose=False)\n",
    "                nuevo_estado = tuple(tuple(part) if isinstance(part, list) else part for part in nuevo_estado)\n",
    "\n",
    "                if estado not in self.q_table:\n",
    "                    self.q_table[estado] = {'plantarse': 0, 'tirar': 0}\n",
    "                \n",
    "                if nuevo_estado not in self.q_table:\n",
    "                    self.q_table[nuevo_estado] = {'plantarse': 0, 'tirar': 0}\n",
    "                    \n",
    "                mejor_q_nuevo_estado = max(self.q_table[nuevo_estado].values())\n",
    "                self.q_table[estado][accion] += self.alpha * (\n",
    "                    recompensa + self.gamma * mejor_q_nuevo_estado - self.q_table[estado][accion]\n",
    "                )\n",
    "                if verbose:\n",
    "                    print(f\"Estado guardado: {estado}, Plantarse/Tirar: {self.q_table[estado]}\")\n",
    "\n",
    "                estado = nuevo_estado\n",
    "                \n",
    "                if self.ambiente.puntaje_total >= 10000:\n",
    "                    terminado = True\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Paso: {episodio + 1}, Estado: {estado}, Acción: {accion}, Recompensa: {recompensa}\")\n",
    "\n",
    "\n",
    "    def guardar_politica(self, filename: str):\n",
    "        # Cargar los estados existentes en el archivo, basado en la cantidad de dados no usados y el puntaje acumulado\n",
    "        estados_existentes = {}\n",
    "        try:\n",
    "            with open(filename, 'r', newline='') as file:\n",
    "                reader = csv.reader(file)\n",
    "                next(reader)  # Saltar el encabezado\n",
    "                for row in reader:\n",
    "                    estado_str = row[0]  # El estado (cantidad de dados y puntaje) está en la primera columna\n",
    "                    plantarse = float(row[1])\n",
    "                    tirar = float(row[2])\n",
    "                    # Convertir el estado en cadena de vuelta a su formato original (evaluación)\n",
    "                    estado_eval = eval(estado_str)\n",
    "                    cantidad_dados, puntaje_acumulado = estado_eval  # Extraer cantidad de dados y puntaje\n",
    "                    # Usamos la cantidad de dados y puntaje como clave\n",
    "                    estados_existentes[(cantidad_dados, puntaje_acumulado)] = {'plantarse': plantarse, 'tirar': tirar}\n",
    "                    \n",
    "        except FileNotFoundError:\n",
    "            # Si el archivo no existe, continuar sin cargar estados\n",
    "            pass\n",
    "\n",
    "        # Actualizar los valores de la política con los de la q_table\n",
    "        for estado, acciones in self.q_table.items():\n",
    "            # Calcular la cantidad de dados no usados y el puntaje actual\n",
    "            cantidad_dados_no_usados = len(estado[0])  # Asumimos que el primer elemento del estado es la lista de dados\n",
    "            puntaje_actual = estado[1]  # Asumimos que el segundo elemento del estado es el puntaje acumulado\n",
    "\n",
    "            clave_estado = (cantidad_dados_no_usados, puntaje_actual)  # Crear la nueva clave basada en la cantidad de dados y puntaje\n",
    "\n",
    "            if clave_estado in estados_existentes:\n",
    "                # Si el estado ya existe, acumular las recompensas\n",
    "                for accion in ['plantarse', 'tirar']:\n",
    "                    # Acumular las recompensas usando la fórmula\n",
    "                    estados_existentes[clave_estado][accion] += self.alpha * (\n",
    "                        acciones[accion] + self.gamma * max(estados_existentes[clave_estado].values())\n",
    "                        - estados_existentes[clave_estado][accion]\n",
    "                    )\n",
    "            else:\n",
    "                # Si es un estado nuevo, agregarlo a estados_existentes\n",
    "                estados_existentes[clave_estado] = acciones\n",
    "\n",
    "        # Abrir el archivo en modo de escritura para actualizar el archivo sin duplicados\n",
    "        with open(filename, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            # Escribir encabezado\n",
    "            writer.writerow(['estado', 'plantarse', 'tirar'])\n",
    "\n",
    "            # Reescribir el archivo con todos los estados, actualizados o no\n",
    "            for clave_estado, acciones in estados_existentes.items():\n",
    "                writer.writerow([str(clave_estado), acciones['plantarse'], acciones['tirar']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JugadorEntrenado(Jugador):\n",
    "    def __init__(self, nombre: str, filename_politica: str):\n",
    "        self.nombre = nombre\n",
    "        self.politica = self._leer_politica(filename_politica)\n",
    "\n",
    "    def _leer_politica(self, filename: str, SEP: str = ','):\n",
    "        \"\"\"Carga una política entrenada de un archivo CSV y la almacena en un diccionario.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre/Path del archivo que contiene a una política almacenada.\n",
    "        \"\"\"\n",
    "        politica = {}\n",
    "        try:\n",
    "            with open(filename, 'r', newline='') as file:\n",
    "                reader = csv.reader(file, delimiter=SEP)\n",
    "                next(reader)  # Saltar el encabezado\n",
    "                for row in reader:\n",
    "                    estado_str = row[0]  # El estado (cantidad de dados y puntaje)\n",
    "                    plantarse = float(row[1])  # Valor para la acción de plantarse\n",
    "                    tirar = float(row[2])  # Valor para la acción de tirar\n",
    "                    \n",
    "                    # Convertir el estado de cadena a tupla (cantidad de dados, puntaje acumulado)\n",
    "                    estado = eval(estado_str)\n",
    "                    politica[estado] = {'plantarse': plantarse, 'tirar': tirar}\n",
    "                    \n",
    "        except FileNotFoundError:\n",
    "            print(f\"El archivo {filename} no se encontró.\")\n",
    "        return politica\n",
    "\n",
    "    def jugar(self, puntaje_total: int, puntaje_turno: int, dados: list[int]) -> tuple[int, list[int]]:\n",
    "        \"\"\"\n",
    "        Devuelve una jugada (plantarse o tirar) y los dados a tirar.\n",
    "        \n",
    "        Args:\n",
    "            puntaje_total (int): Puntaje total del jugador en la partida.\n",
    "            puntaje_turno (int): Puntaje acumulado en el turno del jugador.\n",
    "            dados (list[int]): Tirada del turno actual.\n",
    "\n",
    "        Returns:\n",
    "            tuple[int, list[int]]: Acción a realizar y la lista de dados a tirar.\n",
    "        \"\"\"\n",
    "        # Calcular la cantidad de dados no usados\n",
    "        cantidad_dados_no_usados = len(dados)\n",
    "\n",
    "        # Crear la clave para buscar en la política (cantidad de dados no usados, puntaje_turno)\n",
    "        estado_clave = (cantidad_dados_no_usados, puntaje_turno)\n",
    "\n",
    "        # Consultar la política para determinar la acción\n",
    "        if estado_clave in self.politica:\n",
    "            acciones = self.politica[estado_clave]\n",
    "            # Tomar la acción con el valor más alto\n",
    "            accion = max(acciones, key=acciones.get)\n",
    "        else:\n",
    "            print(f\"Estado {estado_clave} no encontrado en la política, usando acción por defecto.\")\n",
    "            accion = 'plantarse'  # Acción por defecto si no se encuentra el estado\n",
    "\n",
    "        # Realizar la acción correspondiente\n",
    "        if accion == 'plantarse':\n",
    "            return ('plantarse', [])\n",
    "        elif accion == 'tirar':\n",
    "            # Aquí puedes calcular los dados no usados\n",
    "            # Si decides tirar, devolver los dados no usados\n",
    "            puntaje, no_usados = puntaje_y_no_usados(dados)\n",
    "            return ('tirar', no_usados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiente = AmbienteDiezMil()\n",
    "agente = AgenteQLearning(ambiente, alpha=0.1, gamma=0.9, epsilon=0.2)\n",
    "agente.entrenar(episodios=100, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardado de Política"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente.guardar_politica(\"politica_qlearning.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jugador = JugadorEntrenado(\"Agente Trained\", \"politica_qlearning.csv\")\n",
    "print(jugador.politica)\n",
    "resultado = jugador.jugar(puntaje_total=0, puntaje_turno=0, dados=[randint(1, 6) for _ in range(6)])\n",
    "\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculo promedio de varias partidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hiperparametros:\n",
    "- alpha=0.1\n",
    "- gamma=0.9\n",
    "- epsilon=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Definir el número de partidas\n",
    "numero_partidas = 1000\n",
    "\n",
    "# Almacenar los puntajes y el número de pasos de cada partida\n",
    "puntajes_finales = []\n",
    "episodios_por_partida = []\n",
    "\n",
    "for _ in tqdm(range(numero_partidas)):\n",
    "    # Crear una nueva instancia del ambiente y del agente para cada partida\n",
    "    ambiente = AmbienteDiezMil()\n",
    "    agente = AgenteQLearning(ambiente, alpha=0.1, gamma=0.9, epsilon=0.2)\n",
    "    \n",
    "    # Entrenar el agente en una partida de 100 episodios\n",
    "    agente.entrenar(episodios=100, verbose=False)\n",
    "    \n",
    "    # Guardar el puntaje final de la partida y el número de pasos\n",
    "    puntajes_finales.append(ambiente.puntaje_total)\n",
    "    episodios_por_partida.append(ambiente.episodios_totales)  # Asumiendo que `ambiente.pasos_totales` guarda los pasos realizados\n",
    "\n",
    "    agente.guardar_politica(\"politica_qlearning.csv\")\n",
    "\n",
    "# Calcular el promedio de puntajes y el promedio de pasos\n",
    "promedio_puntajes = np.mean(puntajes_finales)\n",
    "promedio_pasos = np.mean(episodios_por_partida)\n",
    "\n",
    "print(f\"Promedio de puntajes después de {numero_partidas} partidas: {promedio_puntajes}\")\n",
    "print(f\"Promedio de pasos por partida después de {numero_partidas} partidas: {promedio_pasos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Promedio de puntajes después de {numero_partidas} partidas: {promedio_puntajes}\")\n",
    "print(f\"Promedio de pasos por partida después de {numero_partidas} partidas: {promedio_pasos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(puntajes_finales)\n",
    "print(episodios_por_partida)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jugador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jugador = JugadorEntrenado('Pepito', 'politica_qlearning.csv')\n",
    "juego = JuegoDiezMil(jugador)\n",
    "cantidad_turnos, puntaje_final = juego.jugar(verbose=True)\n",
    "\n",
    "print(f\"Cantidad de turnos: {cantidad_turnos}\")\n",
    "print(f\"Puntaje final: {puntaje_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir variables acumuladoras\n",
    "total_turnos = 0\n",
    "numero_simulaciones = 1000\n",
    "\n",
    "# Bucle para correr el juego 1000 veces\n",
    "for _ in range(numero_simulaciones):\n",
    "    jugador = JugadorEntrenado('Pepito', 'politica_qlearning.csv')\n",
    "    juego = JuegoDiezMil(jugador)\n",
    "    \n",
    "    # Jugar una partida\n",
    "    cantidad_turnos, puntaje_final = juego.jugar(verbose=False) \n",
    "    \n",
    "    # Sumar la cantidad de turnos al total\n",
    "    total_turnos += cantidad_turnos\n",
    "\n",
    "# Calcular el promedio de turnos\n",
    "promedio_turnos = total_turnos / numero_simulaciones\n",
    "\n",
    "# Imprimir el resultado\n",
    "print(f\"El promedio de la cantidad de turnos en {numero_simulaciones} juegos es: {promedio_turnos}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
