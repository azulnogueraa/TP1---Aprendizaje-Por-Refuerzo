{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1 - Aprendizaje Por Refuerzos\n",
    "## Integrantes:\n",
    "- **Azul Noguera** \n",
    "- **Paula Jordan** \n",
    "- **Rocio Gonzalez**\n",
    "- **Valentina Pancaldi**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enunciado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar un agente de aprendizaje por refuerzo y un ambiente en el cual el agente puede entrenarse. Esto incluye definir la noción de estado del ambiente, que puede no coincidir con el estado del juego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación del Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos las clases\n",
    "from utils import *\n",
    "from jugador import * # importamos la clase Jugador\n",
    "from diezmil import * # importamos la clase DiezMil\n",
    "\n",
    "# importamos funciones de utilidad\n",
    "from utils import puntaje_y_no_usados, separar, JUGADA_PLANTARSE, JUGADA_TIRAR, JUGADAS_STR\n",
    "\n",
    "# importamos las librerias necesarias\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from random import randint\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EstadoDiezMil:\n",
    "    def __init__(self):\n",
    "        \"\"\"Definir qué hace a un estado de diez mil.\n",
    "        Recordar que la complejidad del estado repercute en la complejidad de la tabla del agente de q-learning.\n",
    "        \"\"\"\n",
    "        self.dados = []  # Lista de los valores actuales de los dados\n",
    "        self.puntaje_actual = 0  # Puntaje acumulado en el turno actual\n",
    "        self.turno_terminado = False  # Flag para indicar si el turno ha terminado\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reinicia el estado del juego a sus valores iniciales.\"\"\"\n",
    "        self.dados = self.lanzar_dados(6)  # Reiniciar los dados\n",
    "        self.puntaje_actual = 0  # Reiniciar el puntaje acumulado\n",
    "        self.turno_terminado = False  # Reiniciar el flag de turno terminado\n",
    "\n",
    "    def lanzar_dados(self, cantidad: int) -> list[int]:\n",
    "        \"\"\"Simula el lanzamiento de una cantidad específica de dados.\n",
    "        Args:\n",
    "            cantidad (int): La cantidad de dados a lanzar.\n",
    "        Returns:\n",
    "            list[int]: Los valores obtenidos en los dados lanzados.\n",
    "        \"\"\"\n",
    "        return [randint(1, 6) for _ in range(cantidad)]\n",
    "\n",
    "    def obtener_estado(self):\n",
    "        \"\"\"Devuelve el estado actual del juego en un formato que el agente pueda utilizar.\"\"\"\n",
    "        return (self.dados, self.puntaje_actual, self.turno_terminado)\n",
    "\n",
    "    def actualizar_estado(self, nuevos_dados: list[int], puntaje: int) -> None:\n",
    "        \"\"\"Modifica las variables internas del estado luego de una tirada.\n",
    "        Args:\n",
    "            nuevos_dados (list[int]): Dados no usados tras la tirada.\n",
    "            puntaje (int): Puntaje obtenido en la tirada.\n",
    "        \"\"\"\n",
    "    \n",
    "        self.dados = nuevos_dados\n",
    "        self.puntaje_actual += puntaje\n",
    "        if len(self.dados) == 0:\n",
    "            # Si no quedan dados no usados, el turno debe terminar\n",
    "            self.turno_terminado = True\n",
    "    \n",
    "    def fin_turno(self):\n",
    "        \"\"\"Modifica el estado al terminar el turno.\n",
    "        \"\"\"\n",
    "        self.turno_terminado = True  # Marca el turno como terminado\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Representación en texto de EstadoDiezMil.\n",
    "        Ayuda a tener una versión legible del objeto.\n",
    "\n",
    "        Returns:\n",
    "            str: Representación en texto de EstadoDiezMil.\n",
    "        \"\"\"\n",
    "        estado_str = f\"Dados: {self.dados}, Puntaje Actual: {self.puntaje_actual}, Turno Terminado: {self.turno_terminado}\"\n",
    "        return estado_str   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmbienteDiezMil:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Definir las variables de instancia de un ambiente.\n",
    "        ¿Qué es propio de un ambiente de 10.000?\n",
    "        \"\"\"\n",
    "        self.estado = EstadoDiezMil()\n",
    "        self.turno_terminado = False  # flag que indica si el turno terminó\n",
    "        self.puntaje_total = 0  # Mantener un registro del puntaje total\n",
    "        self.pasos_totales = 0  # Contador de pasos\n",
    "        self.episodios_totales = 0  # Contador de episodios\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reinicia el estado del ambiente.\n",
    "        \"\"\"\n",
    "        if self.pasos_totales > 0:  # Solo sumar episodios si se han realizado pasos\n",
    "            self.episodios_totales += 1\n",
    "        self.estado.reset()\n",
    "        self.turno_terminado = False\n",
    "        self.pasos_totales = 0\n",
    "        \n",
    "        # Aquí no reseteamos `self.puntaje_total` para mantener el acumulado entre episodios.\n",
    "        return self.estado.obtener_estado()\n",
    "    \n",
    "    def step(self, accion):\n",
    "        \"\"\"\n",
    "        Dada una acción devuelve una recompensa.\n",
    "        El estado es modificado acorde a la acción y su interacción con el ambiente.\n",
    "        Podría ser útil devolver si terminó o no el turno.\n",
    "        \n",
    "        Args:\n",
    "            accion: Acción elegida por un agente.\n",
    "        \n",
    "        Returns:\n",
    "            tuple[int, bool]: Una recompensa y un flag que indica si terminó el turno. \n",
    "        \"\"\"\n",
    "        # print(f\"\\n{'='*50}\")\n",
    "        # print(f\"Acción Elegida: {accion.upper()}\")\n",
    "\n",
    "        # print(f\"Estado Antes de la Acción: {self.estado}\")\n",
    "        \n",
    "        self.pasos_totales += 1  # Incrementar el contador de pasos en cada acción\n",
    "        # print(f\"Paso {self.pasos_totales}: Acción = {accion}, Puntaje Total = {self.puntaje_total}\")\n",
    "        \n",
    "        if accion == 'plantarse':\n",
    "            self.turno_terminado = True\n",
    "            recompensa = self.estado.puntaje_actual\n",
    "            self.puntaje_total += recompensa  # Acumular la recompensa al puntaje total\n",
    "            \n",
    "        elif accion == 'tirar':\n",
    "            nuevos_dados = self.estado.lanzar_dados(len(self.estado.dados))\n",
    "            puntaje, dados_no_usados = puntaje_y_no_usados(nuevos_dados)\n",
    "            # print(f\"Tirada de Dados: {nuevos_dados}\")\n",
    "            # print(f\"Puntaje Calculado: {puntaje}, Dados No Usados: {dados_no_usados}\")\n",
    "            self.estado.actualizar_estado(dados_no_usados, puntaje)\n",
    "            recompensa = self.estado.puntaje_actual\n",
    "            self.puntaje_total += recompensa  # Acumular la recompensa al puntaje total\n",
    "            if recompensa == 0:\n",
    "                self.turno_terminado = True\n",
    "        else:\n",
    "            raise ValueError(\"Acción no válida.\")\n",
    "        \n",
    "        # print(f\"Estado Después de la Acción: {self.estado}\")\n",
    "        # print(f\"Recompensa Obtenida: {recompensa}\")\n",
    "        # print(f\"Puntaje Total Acumulado: {self.puntaje_total}\")\n",
    "        # print(f\"{'='*50}\\n\")\n",
    "        \n",
    "        return self.estado.obtener_estado(), recompensa, self.turno_terminado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgenteQLearning:\n",
    "    def __init__(self, ambiente: AmbienteDiezMil, alpha: float, gamma: float, epsilon: float):\n",
    "        self.ambiente = ambiente\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = {}\n",
    "\n",
    "    def elegir_accion(self, estado):\n",
    "        # Convertir cada parte del estado a una tupla si es necesario\n",
    "        estado = tuple(tuple(part) if isinstance(part, list) else part for part in estado)\n",
    "        \n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(['plantarse', 'tirar'])\n",
    "        else:\n",
    "            if estado in self.q_table:\n",
    "                return max(self.q_table[estado], key=self.q_table[estado].get)\n",
    "            else:\n",
    "                return random.choice(['plantarse', 'tirar'])\n",
    "\n",
    "    def entrenar(self, episodios: int, verbose: bool = False) -> None:\n",
    "        for episodio in tqdm(range(episodios)):\n",
    "            estado = self.ambiente.reset()\n",
    "            estado = tuple(tuple(part) if isinstance(part, list) else part for part in estado)\n",
    "            terminado = False\n",
    "\n",
    "            while not terminado:\n",
    "                accion = self.elegir_accion(estado)\n",
    "                nuevo_estado, recompensa, terminado = self.ambiente.step(accion)\n",
    "                nuevo_estado = tuple(tuple(part) if isinstance(part, list) else part for part in nuevo_estado)\n",
    "\n",
    "                if estado not in self.q_table:\n",
    "                    self.q_table[estado] = {'plantarse': 0, 'tirar': 0}\n",
    "                \n",
    "                if nuevo_estado not in self.q_table:\n",
    "                    self.q_table[nuevo_estado] = {'plantarse': 0, 'tirar': 0}\n",
    "\n",
    "                mejor_q_nuevo_estado = max(self.q_table[nuevo_estado].values())\n",
    "                self.q_table[estado][accion] += self.alpha * (\n",
    "                    recompensa + self.gamma * mejor_q_nuevo_estado - self.q_table[estado][accion]\n",
    "                )\n",
    "\n",
    "                estado = nuevo_estado\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Paso: {episodio + 1}, Estado: {estado}, Acción: {accion}, Recompensa: {recompensa}\")\n",
    "\n",
    "            # # Verificar la condición de finalización del juego\n",
    "            if self.ambiente.puntaje_total >= 10000:\n",
    "                print(f\"Entrenamiento terminado en el episodio {episodio + 1} con un puntaje total de {self.ambiente.puntaje_total}.\")\n",
    "                break\n",
    "\n",
    "    def guardar_politica(self, filename: str):\n",
    "        with open(filename, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            # Guardar encabezado\n",
    "            writer.writerow(['estado', 'plantarse', 'tirar'])\n",
    "            for estado, acciones in self.q_table.items():\n",
    "                # Ordenar los dados dentro del estado antes de convertirlo en cadena\n",
    "                dados_ordenados = tuple(sorted(estado[0]))  # Suponiendo que los dados son el primer elemento del estado\n",
    "                estado_ordenado = (dados_ordenados,) + estado[1:]  # Recombina el estado con los dados ordenados\n",
    "                estado_str = str(estado_ordenado)  # Convertir el estado a una cadena para almacenarlo en el CSV\n",
    "                writer.writerow([estado_str, acciones['plantarse'], acciones['tirar']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JugadorEntrenado(Jugador):\n",
    "    def __init__(self, nombre: str, filename_politica: str):\n",
    "        self.nombre = nombre\n",
    "        self.politica = self._leer_politica(filename_politica)\n",
    "        \n",
    "    def _leer_politica(self, filename:str, SEP:str=','):\n",
    "        \"\"\"Carga una politica entrenada con un agente de RL, que está guardada\n",
    "        en el archivo filename en formato CSV.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre/Path del archivo que contiene a una política almacenada. \n",
    "        \"\"\"\n",
    "        politica = {}\n",
    "        with open(filename, 'r') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for row in reader:\n",
    "                estado = eval(row['estado'])  \n",
    "                politica[estado] = {\n",
    "                    'plantarse': float(row['plantarse']),\n",
    "                    'tirar': float(row['tirar'])\n",
    "                }\n",
    "        return politica\n",
    "    \n",
    "    def jugar(self, puntaje_total:int, puntaje_turno:int, dados:list[int]) -> tuple[int,list[int]]:\n",
    "        \"\"\"\n",
    "        Devuelve una jugada y los dados a tirar.\n",
    "        Args:\n",
    "            puntaje_total (int): Puntaje total del jugador en la partida.\n",
    "            puntaje_turno (int): Puntaje en el turno del jugador\n",
    "            dados (list[int]): Tirada del turno.\n",
    "        Returns:\n",
    "            tuple[int,list[int]]: Una jugada y la lista de dados a tirar.\n",
    "        \"\"\"\n",
    "\n",
    "        # Calcular el puntaje actual y los dados no usados\n",
    "        puntaje, no_usados = puntaje_y_no_usados(dados)\n",
    "\n",
    "        # Definir el estado como una tupla con los componentes relevantes\n",
    "        estado = (tuple(dados), puntaje_turno, False)  # Matching the structure used in training\n",
    "        \n",
    "        # Consultar la política para determinar la jugada\n",
    "        jugada = self.politica.get(estado, None) \n",
    "        if jugada is not None:\n",
    "            accion = max(jugada, key=jugada.get)  # Choose the action with the highest value\n",
    "        else:\n",
    "            print(f\"Estado {estado} no encontrado en la política, usando acción por defecto.\")\n",
    "            accion = 'plantarse'  # Default action if the state is not found\n",
    "\n",
    "        if accion == 'plantarse':\n",
    "            return ('plantarse', [])\n",
    "        elif accion == 'tirar':\n",
    "            return ('tirar', no_usados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiente = AmbienteDiezMil()\n",
    "agente = AgenteQLearning(ambiente, alpha=0.1, gamma=0.9, epsilon=0.2)\n",
    "agente.entrenar(episodios=100, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardado de Política"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente.guardar_politica(\"politica_qlearning.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jugador = JugadorEntrenado(\"Agente Trained\", \"politica_qlearning.csv\")\n",
    "print(jugador.politica)\n",
    "resultado = jugador.jugar(puntaje_total=0, puntaje_turno=0, dados=[4, 2, 5, 2, 1, 5])\n",
    "\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculo promedio de varias partidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hiperparametros:\n",
    "- alpha=0.1\n",
    "- gamma=0.9\n",
    "- epsilon=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Definir el número de partidas\n",
    "numero_partidas = 1000  # Puedes ajustar este número según tus necesidades\n",
    "\n",
    "# Almacenar los puntajes y el número de pasos de cada partida\n",
    "puntajes_finales = []\n",
    "episodios_por_partida = []\n",
    "\n",
    "for _ in tqdm(range(numero_partidas)):\n",
    "    # Crear una nueva instancia del ambiente y del agente para cada partida\n",
    "    ambiente = AmbienteDiezMil()\n",
    "    agente = AgenteQLearning(ambiente, alpha=0.1, gamma=0.9, epsilon=0.2)\n",
    "    \n",
    "    # Entrenar el agente en una partida de 100 episodios\n",
    "    agente.entrenar(episodios=100, verbose=False)\n",
    "    \n",
    "    # Guardar el puntaje final de la partida y el número de pasos\n",
    "    puntajes_finales.append(ambiente.puntaje_total)\n",
    "    episodios_por_partida.append(ambiente.episodios_totales)  # Asumiendo que `ambiente.pasos_totales` guarda los pasos realizados\n",
    "\n",
    "# Guardar la política después de todas las partidas\n",
    "agente.guardar_politica(\"politica_qlearning.csv\")\n",
    "\n",
    "# Calcular el promedio de puntajes y el promedio de pasos\n",
    "promedio_puntajes = np.mean(puntajes_finales)\n",
    "promedio_pasos = np.mean(episodios_por_partida)\n",
    "\n",
    "print(f\"Promedio de puntajes después de {numero_partidas} partidas: {promedio_puntajes}\")\n",
    "print(f\"Promedio de pasos por partida después de {numero_partidas} partidas: {promedio_pasos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Promedio de puntajes después de {numero_partidas} partidas: {promedio_puntajes}\")\n",
    "print(f\"Promedio de pasos por partida después de {numero_partidas} partidas: {promedio_pasos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(puntajes_finales)\n",
    "print(episodios_por_partida)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
